{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Science\n",
    "\n",
    "[Gina Sprint](https://ginasprint.com/)\n",
    "\n",
    "# Classifier Evaluation\n",
    "What are our learning objectives for this lesson?\n",
    "* Evaluate classifier performance using different metrics\n",
    "* Divided a dataset into training and testing sets using different approaches\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm up Task(s)\n",
    "Open our T-shirt example notes from last class\n",
    "1. Normalize the weight values using min-max scaling (see our work with normalizing the height column for an example)\n",
    "1. Calculate the distance between each training instance and the unseen instance using $distance(a, b) = \\sqrt{\\sum (a_i - b_i)^2}$\n",
    "1. Which k=3 training instances have the smallest distances?\n",
    "\n",
    "## Today\n",
    "1. Finish our k nearest neighbors algorithm example\n",
    "1. Code up our k nearest neighbors algorithm in ClassificationFun\n",
    "1. Break\n",
    "1. Finish ClassificationFun (train/test split and accuracy)\n",
    "1. Go over project part 4\n",
    "\n",
    "## TODO\n",
    "1. Review all of the \"fun\" in-class examples we did\n",
    "    1. PythonBasicsFun\n",
    "    1. PandasFun\n",
    "    1. DataCleaningFun\n",
    "    1. DataVisualizationFun\n",
    "    1. JupyterNotebookFun\n",
    "    1. ClassificationFun\n",
    "1. Work on project part 4 and project part 3 (BONUS)\n",
    "    * Note: I moved project part 3 to be BONUS (extra credit) ðŸ˜€\n",
    "    * Note: If you are unable to get all code working for the project, please write Python comments to describe what you tried and what the code should do\n",
    "1. Turn in the project by providing your Github repository URL link in Moodle\n",
    "\n",
    "## Thank you for a great first time teaching in China!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "### Accuracy\n",
    "Accuracy: % of test instances correctly classified by the classifier\n",
    "* Sometimes called \"recognition rate\"\n",
    "* Use the [KNeighborsClassifier.score(X, y)](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.score) method to determine the accuracy of a kNN classifier \n",
    "        * Description: \"Return the mean accuracy on the given test data and labels.\"\n",
    "    * Note: for other classifiers, check the documentation for the `score()` method to see what the default evaluation metric is\n",
    "* Warning: can be skewed if unbalanced distribution of class labels\n",
    "    * e.g., lots of negative cases that are easily detected (e.g. 99% accuracy when 99% of the dataset is the negative class)\n",
    "    * shadows performance on positive cases\n",
    "\n",
    "### More Classifier Evaluation Metrics to Look Into\n",
    "* Error rate: 1 - accuracy\n",
    "* Precision: measure of \"exactness\"\n",
    "* Recall (AKA sensitivity): measure of \"completeness\"\n",
    "* F-Measure (AKA F1 score): combine the two via the harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "Building a classifier starts with a learning (training) phase\n",
    "* Based on predefined set of examples (AKA the training set)\n",
    "\n",
    "The classifier is then evaluated for predictive accuracy\n",
    "* Based on another set of examples (AKA the testing set)\n",
    "* We use the actual labels of the examples to test the predictions\n",
    "\n",
    "In general, we want to try to avoid overfitting\n",
    "* That is, encoding particular characteristics/anomalies of the training set into the classifier\n",
    "* Similar notion is \"underfitting\" (too simple of a model, e.g., linear instead of polynomial)\n",
    "\n",
    "There are several different ways to select training and testing sets:\n",
    "1. The train/test split (holdout) method\n",
    "    * The one we will go over\n",
    "2. Random subsampling\n",
    "3. $k$-Fold cross validation and variants\n",
    "4. Bootstrap method\n",
    "\n",
    "### Train/Test Split (Holdout) Method\n",
    "In the train/test split method, the dataset is divided into two sets, the training and the testing set. The training set is used to build the model and the testing set is used to evaluate the model (e.g. the model's accuracy).\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Supervised_machine_learning_in_a_nutshell.svg/2000px-Supervised_machine_learning_in_a_nutshell.svg.png)\n",
    "(image from https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Supervised_machine_learning_in_a_nutshell.svg/2000px-Supervised_machine_learning_in_a_nutshell.svg.png)\n",
    "\n",
    "Approaches to the train/test split method\n",
    "* Randomly divide data set into a training and test set\n",
    "* Partition evenly or, e.g., $\\frac{2}{3}$ to $\\frac{1}{3}$ (2:1) training to test set\n",
    "* This is random selection without replacement\n",
    "* Use the [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to apply the holdout method to a dataset\n",
    "    * Description: \"Split arrays or matrices into random train and test subsets\"\n",
    "    * `test_size` parameter: \"If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If train_size is also None, it will be set to 0.25\"\n",
    "    * `random_state` parameter: \"Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls.\"\n",
    "    * `stratify` parameter: \"If not None, data is split in a stratified fashion, using this as the class labels.\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Science\n",
    "\n",
    "[Gina Sprint](https://ginasprint.com/)\n",
    "\n",
    "# Introduction to Machine Learning\n",
    "What are our learning objectives for this lesson?\n",
    "* Understand what machine learning is\n",
    "* Revisit the concept of labeled and unlabeled data\n",
    "* Understand the difference between supervised and unsupervised machine learning\n",
    "* Understand the difference between classification and regression\n",
    "* Learn about the kNN classification algorithm\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm up Task(s)\n",
    "Open our MarkdownBasics.ipynb from last class: https://github.com/gsprint23/ZIME-Intro-to-Data-Science/blob/master/JupyterNotebookFun\n",
    "1. Using Latex, typeset the quadratic formula:  \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Quadratic_formula.svg/2560px-Quadratic_formula.svg.png\" width=\"200\"/>\n",
    "1. Then, write a code cell with a function to implement it :)\n",
    "\n",
    "## Today\n",
    "1. A data storytelling with Jupyter Notebook example: https://github.com/gsprint23/ZIME-Intro-to-Data-Science/blob/master/JupyterNotebookFun/DataStorytellingExample.ipynb\n",
    "1. Overview of k nearest neighbors algorithm\n",
    "1. Break\n",
    "1. Start ClassificationFun\n",
    "\n",
    "## TODO\n",
    "1. Work on Quiz 5 in Moodle\n",
    "1. Work on project part 2 and project part 3 (BONUS)\n",
    "    * Note: I moved project part 3 to be BONUS (extra credit) ðŸ˜€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "At a high level, machine learning is building and using models that are learned from data. Machine learning is a subset of artificial intelligence, and it greatly overlaps with data mining. Let's see the \"unofficial\" definitions for these areas from Wikipedia:\n",
    "* [Data mining](https://en.wikipedia.org/wiki/Data_mining): The computational process of discovering patterns in large data sets involving methods at the intersection of artificial intelligence, machine learning, statistics, and database systems. It is an interdisciplinary subfield of computer science. The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use.\n",
    "    * Take away point: Discovering and using patterns in data\n",
    "* [Artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence): The study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of success at some goal. Colloquially, the term \"artificial intelligence\" is applied when a machine mimics \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\" (known as Machine Learning).\n",
    "    * Take away point: Implementing human-cognition on a machine\n",
    "* [Machine learning](https://en.wikipedia.org/wiki/Machine_learning): The subfield of computer science that, according to Arthur Samuel in 1959, gives \"computers the ability to learn without being explicitly programmed.\" Evolved from the study of pattern recognition and computational learning theory in artificial intelligence, machine learning explores the study and construction of algorithms that can learn from and make predictions on data â€“ such algorithms overcome following strictly static program instructions by making data driven predictions or decisions, through building a model from sample inputs.\n",
    "    * Take away point: Learning from and making predictions on data\n",
    "    \n",
    "Here is an overview of the different components and applications of machine learning: \n",
    "\n",
    "![](https://miro.medium.com/max/1113/1*6iDmbHflsN6NULBLpVHA6A.png)\n",
    "\n",
    "(image from https://miro.medium.com/max/1113/1*6iDmbHflsN6NULBLpVHA6A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "Supervised learning requires \"labeled\" training data from a \"supervisor.\" Such labels are considered the ground-truth for describing the data. The label comes from a knowledgeable expert and can be used to learn what information describes different labels.\n",
    "* If the labeled attribute is categorical, then the learning task is called \"classification\"\n",
    "* If the labeled attribute is continuous, then the learning task is called \"regression\"\n",
    "\n",
    "Supervised learning is typically composed of training and testing. We will train a machine (AKA a student, learner, mathematical model) to learn a concept. Then we will test the machine's learned concept by applying their knowledge.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Supervised_machine_learning_in_a_nutshell.svg/2000px-Supervised_machine_learning_in_a_nutshell.svg.png\" width=\"650\">\n",
    "\n",
    "(image from [https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Supervised_machine_learning_in_a_nutshell.svg/2000px-Supervised_machine_learning_in_a_nutshell.svg.png](https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Supervised_machine_learning_in_a_nutshell.svg/2000px-Supervised_machine_learning_in_a_nutshell.svg.png))\n",
    "\n",
    "### Training\n",
    "As an example, suppose you are trying to teach someone (say a student) who has no notion of a cat or dog, the concept of cat vs. dog. You might first show the student some pictures of cats and say, \"these are cats\". Then you might show the person some pictures of dogs and say, \"these are dogs\". The set of cat and dog images is called the *training set*, a set of labeled examples (e.g. *instances*). For example, consider the following cat vs. dog training set:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gsprint23/cpts215/master/lessons/figures/cat_dog_training.png\" width=\"500\"/>\n",
    "\n",
    "The student is going to look at different attributes of the image to try to learn a model of cat and a model of a dog. In doing so, the student will identify some aspects (AKA *attributes* or *features*) of the examples that distinguish a cat vs a dog. The features might include:\n",
    "\n",
    "|Feature|Cat value|Dog value|\n",
    "|-|-|-|\n",
    "|Tongue out|No|Yes|\n",
    "|Fur color|Light|Dark|\n",
    "|Ears up|Yes|No|\n",
    "\n",
    "What other features did you come up with?\n",
    "\n",
    "#### Building a Model\n",
    "A model to represent cat vs. dog based on these features might be rule-based:\n",
    "\n",
    ">if tongue is out and the fur is dark and the ears are down then this is a dog\n",
    "\n",
    "We will see later how we can use a tree with a rules (like the above) as a model to represent a classification such as dog vs. cat!\n",
    "\n",
    "### Testing\n",
    "Now, suppose we want to apply the student's learned conception of dog vs. cat by providing the student with a new, unseen example:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gsprint23/cpts215/master/lessons/figures/cat_or_dog.png\" width=\"150\"/>\n",
    "\n",
    "Based on the above features, this image has the tongue out (dog), light fur color (cat), and ears up (cat). Thus our student would likely classify this image as a cat. But wait! We (the expert supervisors) know this is a dog (a puppy, but a dog none the less). Our training set didn't include any images that were as borderline cat/dog as this testing example. As you can see, the examples that comprise your training set and the features that are utilized greatly impact the accuracy of the learner, and consequently the model that is built. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Regression\n",
    "The basic of idea of classification is:\n",
    "* Given a data set (samples) and a new, unclassified instance\n",
    "* Try to predict its classification (based on samples)\n",
    "\n",
    "Note regression can be used in a similar way ... Let's say we have: $y = mx + b$\n",
    "\n",
    "Q: How do we use this on a new instance?\n",
    "* Predict a new $y'$ value from a new, unseen instance $x_{unseen}$ by calculating $y' = mx_{unseen} + b$\n",
    "\n",
    "Approaches we will look at to classification\n",
    "* k Nearest Neighbor (k-NN)... find \"close cases\"\n",
    "* Naive Bayes... select \"most probable\" class for instance\n",
    "* Decision Tree Induction... find \"general\" rules based on entropy\n",
    "* Ensemble Methods... use many approaches to find best class (hybrid)\n",
    "\n",
    "We'll also look at ways to evaluate classification results\n",
    "* These largely involve splitting up a data set into training and testing sets\n",
    "* Plus some basic statistics/metrics for accuracy, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "Unsupervised learning does not require labeled training data. Information learned from the examples is data-driven and includes the process of discovering and describing patterns in the data. \n",
    "\n",
    "For example, to apply unsupervised learning to our cat vs. dog example, we would not try to \"train\" our student to learn the notion of \"cat\" or \"dog\". Instead, we would have our student look for patterns in the data, or perhaps a natural grouping. \n",
    "\n",
    "Here are our cat-dog training examples sorted in order based on the feature fur color:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gsprint23/cpts215/master/lessons/figures/cat_dog_fur_ordering.png\" width=\"500\"/>\n",
    "\n",
    "We could apply a clustering algorithm, such as $k$-means clustering, to the data to reveal two natural groups in the data ($k = 2$):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gsprint23/cpts215/master/lessons/figures/cat_dog_grouping.png\" width=\"500\"/>\n",
    "\n",
    "Note that these two groups, blue and red, are not representative of cat and dog, since we have no cat/dog labels!\n",
    "\n",
    "Now, upon seeing a new instance, we can determine the new instance's membership to either the blue group or the red group:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/gsprint23/cpts215/master/lessons/figures/cat_dog_membership.png\" width=\"500\"/>\n",
    "\n",
    "Like supervised machine learning, there are several unsupervised machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Classification\n",
    "Nearest neighbor classification is typically used when attributes are continuous\n",
    "* Can be modified for categorical data ...\n",
    "\n",
    "Basic approach:\n",
    "* Given an instance $i$ with $n - 1$ attributes (where the $n^{th}$ is class label)\n",
    "* Find the \"closest\" instance $j$ to $i$ on the $n - 1$ attributes\n",
    "* Use $j$'s class as the prediction for $i$\n",
    "\n",
    "Example from [Bramer](https://www.amazon.com/Principles-Mining-Undergraduate-Computer-Science/dp/1447148835):\n",
    "Given the data set\n",
    "\n",
    "|a |b |c |d |e |f |Class|\n",
    "|-|-|-|-|-|-|-|\n",
    "|yes |no |no |6.4 |8.3 |low |negative|\n",
    "|yes |yes |yes |18.2 |4.7 |high |positive|\n",
    "\n",
    "What should this instance's classification be?\n",
    "\n",
    "|yes |no |no |6.6 |8.0 |low |???|\n",
    "|-|-|-|-|-|-|-|\n",
    "\n",
    "Usually it isn't this easy!\n",
    "\n",
    "## k Nearest Neighbors\n",
    "Find the k nearest neighbors ...\n",
    "* Usually find the k closest neighbors (instead of just closest)\n",
    "* Then pick classification from among the top k\n",
    "\n",
    "What are good values for the number of neighbors k?\n",
    "* Often done experimentally (with a test set)\n",
    "* Start with k = 1, determine \"error rate\" (more later)\n",
    "* Repeat incrementing k\n",
    "* Pick k with smallest (minimum) error rate\n",
    "     * Often, larger the data (training) set, the larger the k\n",
    "\n",
    "Lets say we found the k nearest neighbors for an instance ...\n",
    "\n",
    "Q: What are ways we could pick the class?\n",
    "* Most frequent occurring class\n",
    "* Weighted \"average\" (based on the relative closest of the k)\n",
    "\n",
    "Note: can use k-NN for regression if, e.g., return the mean of the label values\n",
    "\n",
    "## Distance Functions\n",
    "k-NN works by calculating distances between instances\n",
    "* Many possible ways to do this ... generalized through \"distance measures\"\n",
    "* For two points $x$ and $y$, the distance between them is given by $dist(x,y)$\n",
    "\n",
    "Properties of distance measures (metrics)\n",
    "1. $\\forall x$, $dist(x,x) = 0$\n",
    "    * The distance of any point x from itself is zero\n",
    "2. $\\forall xy$, $dist(x,y) = dist(y,x)$\n",
    "    * Symmetry\n",
    "3. $\\forall xyz$, $dist(x,y) \\leq dist(x,z) + dist(z,y)$\n",
    "    * Triangle equality\n",
    "    * \"Shortest distance between any two points is a straight line\"\n",
    "\n",
    "Euclidean Distance is most often used: Given an instance, treat it as a \"vector\" in n space\n",
    "* Use Pythagoras' Theorem to find distance between them\n",
    "\n",
    "For example:\n",
    "* Given two points (i.e., rows) with $n = 2$: $(x_1, y_1)$ and $(x_2, y_2)$\n",
    "* The length (i.e., distance) of the straight line joining the points is:\n",
    "\n",
    "$$\\sqrt{(x_1 - x_2)^{2} + (y_1 - y_2)^{2}}$$\n",
    "\n",
    "* Euclidean $n$-space\n",
    "    * For rows A = $(a_1, a_2,..., a_n)$ and $B = (b_1, b_2,..., b_n)$ with $n$ attributes\n",
    "$$\\sqrt{(a_1 - b_1)^{2} + (a_2 - b_2)^{2} +...+ (a_n - b_n)^{2}}$$\n",
    "        * Which is:\n",
    "$$\\sqrt{\\sum_{i=1}^{n}(a_i - b_i)^{2}}$$\n",
    "\n",
    "Other examples are described in the book (e.g., Manhattan \"city block\" distance)\n",
    "\n",
    "Q: Do you see any possible issues with Euclidean distance?\n",
    "* Larger values tend to dominate smaller ones\n",
    "* Can degrade into a few attributes driving the distances\n",
    "    * e.g., [Mileage=18,457, Doors=2, Age=12]\n",
    "    \n",
    "## Normalization\n",
    "One solution is to scale all values between 0 and 1 (\"min-max\" normalization)\n",
    "* Use the formula: `(x - min(xs)) / ((max(xs) - min(xs)) * 1.0)`\n",
    "\n",
    "Q: How can we deal with categorical values?\n",
    "* $dist(v_1, v_2) = 0$ if values $v_1 = v_2$\n",
    "    * Same values\n",
    "* For nominal values, $dist(v_1, v_2) = 1$ if $v_1 \\neq v_2$\n",
    "    * Different values\n",
    "* For ordinal values, assign to 1 or use the \"distance\"\n",
    "\n",
    "Q: What do we do about missing values?\n",
    "* Don't have missing values\n",
    "    * Clean the data first\n",
    "* Be conservative\n",
    "    * Assuming normalized\n",
    "    * If only one value missing:\n",
    "        * Assume the maximum possible distance\n",
    "        * If nominal use the maximum distance (i.e., 1)\n",
    "        * If ordinal use either 1 or furthest distance from known value\n",
    "    * otherwise, if both values missing, use the maximum distance (e.g., 1)\n",
    "\n",
    "Distance-based metrics imply equal weighting of attributes\n",
    "* Sometimes can perform better with attribute weights (i.e., some attributes worth more)\n",
    "* \"Feature reduction\" (not using certain attributes) can also help with redundant or \"noisy\" attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic k-NN Algorithm\n",
    "```\n",
    "Input: list of rows, no of atts (n where nth is label), instance to classify, k\n",
    "def kNN_classifier(training_set, n, instance, k):\n",
    "    row_distances = []\n",
    "    for row in training_set:\n",
    "        d = distance(row, instance, n - 1)\n",
    "        row_distances.append([d, row])\n",
    "    top_k_rows = get_top_k(row_distances, k)\n",
    "    label = select_class_label(top_k_rows)\n",
    "    return label\n",
    "```\n",
    "\n",
    "## kNN Example\n",
    "Example adapted from [this kNN example](https://people.revoledu.com/kardi/tutorial/KNN/KNN_Numerical-example.html)\n",
    "\n",
    "Suppose we have the following dataset that has two attributes (acid durability and strength) and a class attribute (whether a special paper tissue is good or not):\n",
    "\n",
    "|Acid durability (seconds)|Strength (kg/square meter)|Classification|\n",
    "|-|-|-|\n",
    "|7|7|Bad|\n",
    "|7|4|Bad|\n",
    "|3|4|Good|\n",
    "|1|4| Good|\n",
    "\n",
    "Now the factory produces a new paper tissue with acid durability = 3 seconds and strength = 7 kg/square meter. Can we predict what the classification of this new tissue is? Use kNN with $k$ = 3. \n",
    "\n",
    "### Make a Prediction Manually\n",
    "Steps:\n",
    "1. Normalize\n",
    "1. Compute distance of each training instance to the test instance\n",
    "1. Determine the majority classification of the $k$ closest instances... this is your prediction for the test instance\n",
    "\n",
    "After normalization:\n",
    "\n",
    "|Acid durability (seconds)|Strength (kg/square meter)|Classification|\n",
    "|-|-|-|\n",
    "|1|1|Bad|\n",
    "|1|0|Bad|\n",
    "|0.33|0|Good|\n",
    "|0|0| Good|\n",
    "\n",
    "Test instance normalization: 0.33, 1\n",
    "\n",
    "Distances:\n",
    "\n",
    "|Acid durability (seconds)|Strength (kg/square meter)|Classification|Distance|\n",
    "|-|-|-|-|\n",
    "|1|1|Bad|0.66|\n",
    "|1|0|Bad|1.203|\n",
    "|0.33|0|Good|1.0|\n",
    "|0|0| Good|1.05|\n",
    "\n",
    "Work:\n",
    "* $\\sqrt{(1-0.33)^2 + (1-1)^2} = 0.66$\n",
    "* $\\sqrt{(1-0.33)^2 + (0-1)^2} = 1.203$\n",
    "* $\\sqrt{(0.33-0.33)^2 + (0-1)^2} = 1.0$\n",
    "* $\\sqrt{(0-0.33)^2 + (0-1)^2} = 1.05$\n",
    "\n",
    "Majority classification: \n",
    "1 Bad (0.66) and 2 Goods (1.0 an 1.05) => Good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Prediction with Scikit-Learn\n",
    "Steps:\n",
    "1. Load data\n",
    "1. Normalize\n",
    "1. Train kNN classifier with training set\n",
    "1. Test kNN classifier on test instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Acid durability (seconds)  Strength (kg/square meter) Classification\n",
      "0                          7                           7            Bad\n",
      "1                          7                           4            Bad\n",
      "2                          3                           4           Good\n",
      "3                          1                           4           Good\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "\n",
    "data = [[7, 7, \"Bad\"], [7, 4, \"Bad\"], [3, 4, \"Good\"], [1, 4, \"Good\"]]\n",
    "df = pd.DataFrame(data, columns=[\"Acid durability (seconds)\", \"Strength (kg/square meter)\", \"Classification\"])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 4.]\n",
      "[7. 7.]\n",
      "[[1.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.33333333 0.        ]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train = df.drop(\"Classification\", axis=1)\n",
    "y_train = df[\"Classification\"]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "print(scaler.data_min_)\n",
    "print(scaler.data_max_)\n",
    "\n",
    "X_train_normalized = scaler.transform(X_train)\n",
    "print(X_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good']\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train_normalized, y_train)\n",
    "\n",
    "# test\n",
    "X_test = pd.Series([3, 7], index=df.columns.drop(\"Classification\"))\n",
    "X_test = scaler.transform([X_test])\n",
    "y_test_prediction = neigh.predict(X_test)\n",
    "print(y_test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Notes\n",
    "Q: What happens if there are ties in the top-k distances (get_top_k)? E.g., which are top 3 in: [[.28,$r_1$],[.33,$r_2$],[.33,$r_3$],[.33,$r_4$],[.37,$r_5$]]?\n",
    "* Different options ... e.g.:\n",
    "    * Randomly select from ties\n",
    "    * Do top-k distances (instead of instances)\n",
    "    * Ignore ties (in case above, just use $r_1$ and $r_2$)\n",
    "\n",
    "Nearest doesn't imply near\n",
    "* top-k instances might not be that close to the instance being classified\n",
    "* Especially true as the number of attributes (\"dimensions\") increases\n",
    "    * An example of the \"curse of dimensionality\"\n",
    "* Again, have to use common sense and an understanding of the dataset\n",
    "\n",
    "### Efficiency issues\n",
    "Q: Is k-NN efficient? Can you find any efficiency issues?\n",
    "* Given a training set with $D$ instances and $k = 1$\n",
    "* $O(D)$ comparisons needed to classify a given instance\n",
    "\n",
    "Q: Can you think of any ways to improve the efficiency?\n",
    "1. Use search trees\n",
    "    * Presort and arrange instances into a search tree\n",
    "    * Can reduce comparisons to $O(log D)$\n",
    "2. Check each training instance in parallel\n",
    "    * Gives $O(1)$ comparisons\n",
    "3. Editing/Pruning\n",
    "    * Filter or remove training tuples that prove useless\n",
    "    * Reduces size of $D$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Science\n",
    "\n",
    "[Gina Sprint](https://ginasprint.com/)\n",
    "\n",
    "\n",
    "# Data Processing\n",
    "What are our learning objectives for this lesson?\n",
    "* Clean data by filling missing values\n",
    "* Perform data aggregation w/split-apply-combine\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm up Task(s)\n",
    "Open our main.py from last class: https://github.com/gsprint23/ZIME-Intro-to-Data-Science/blob/master/PandasFun You'll see that I posted the table practice problem solutions from last class' TODO in there too.\n",
    "1. Read the documentation on Pandas' `groupby()`: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html\n",
    "1. Add code main.py to get only Large group `DataFrame` and print it out\n",
    "1. Concept question: for any groupby, how do you know how many group `DataFrame` objects there are?\n",
    "    1. Challenge: how could you find this out programmatically? (lots of ways to solve this actually)\n",
    "\n",
    "## Today\n",
    "1. Questions on project part 1?\n",
    "1. More in PandasFun (`append()`, `value_counts()`, `sort_values()`, Boolean indexing, etc.)\n",
    "1. Break\n",
    "1. DataCleaningFun\n",
    "\n",
    "## TODO\n",
    "1. Pandas practice problems below (not graded)\n",
    "1. Work on Quiz 3 in Moodle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Practice Problems\n",
    "1. Create a CSV file called basketball.csv with the following data:\n",
    "```\n",
    "NAME,POS,GP,MIN,PTS\n",
    "Andrew Nembhard,G,25,760,272\n",
    "Anton Watson,F,25,448,206\n",
    "Ben Gregg,F,16,104,40\n",
    "Chet Holmgren,C,25,659,361\n",
    "Drew Timme,F,25,675,450\n",
    "Hunter Sallis,G,25,372,119\n",
    "Julian Strawther,G,25,647,308\n",
    "Kaden Perry,F,8,53,14\n",
    "Martynas Arlauskas,G,15,53,12\n",
    "Matthew Lang,G,16,52,17\n",
    "Nolan Hickman,G,25,474,156\n",
    "Rasir Bolton,G,25,654,271\n",
    "Will Graves,G,15,28,11\n",
    "```\n",
    "1. Load this CSV file into a Pandas DataFrame\n",
    "1. Add a new row for Joe Few. Example code:\n",
    "```\n",
    "new_row = pd.Series([\"G\",12,20,1], index=df.columns)\n",
    "new_row.name = \"Joe Few\"\n",
    "df = df.append(new_row)\n",
    "```\n",
    "1. Add a new column for CLASS (e.g. freshman, sophomore, junior, or senior). Here are the values:\n",
    "`[\"Sr\", \"Jr\", \"Fr\", \"Fr\", \"Jr\", \"Fr\", \"So\", \"Fr\", \"Jr\", \"Sr\", \"Fr\", \"Sr\", \"Sr\", \"Fr\"]`\n",
    "1. Create a Pandas Series with the count of each CLASS. E.g. for the modified dataset with Joe Few:\n",
    "```\n",
    "Fr    6\n",
    "Sr    4\n",
    "Jr    3\n",
    "So    1\n",
    "Name: Class, dtype: int64\n",
    "```\n",
    "1. Apply split-apply-combine to the PTS data\n",
    "    * Split on Class\n",
    "    * Apply the mean to the PTSs\n",
    "    * Combine the mean PTSs\n",
    "1. Write code to print out who has the most points per game. Check your work with the ESPN web page (at the time of this writing):\n",
    "\n",
    "![](https://raw.githubusercontent.com/GonzagaCPSC222/U5-Visualizing-Data/master/figures/team_leaders.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "Data analysts spend a surprising amount of time preparing data for analysis. In fact, a survey was conducted found that cleaning big data is the most time-consuming and least enjoyable task data scientists do!\n",
    "<img src=\"https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fblogs-images.forbes.com%2Fgilpress%2Ffiles%2F2016%2F03%2FTime-1200x511.jpg\" width=\"700\">\n",
    "(image from [https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fblogs-images.forbes.com%2Fgilpress%2Ffiles%2F2016%2F03%2FTime-1200x511.jpg](https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fblogs-images.forbes.com%2Fgilpress%2Ffiles%2F2016%2F03%2FTime-1200x511.jpg))\n",
    "\n",
    "The goal of data preprocessing is to produce high-quality data to improve mining results and efficiency\n",
    "\n",
    "At a high level, data preprocessing includes the following steps (these steps are done in any order and often multiple times):\n",
    "1. Data Exploration (basic understanding of meaning, attributes, values, issues)\n",
    "2. Data Reduction (reduce size via aggregation, redundant features, etc.)\n",
    "3. Data Integration (join/merge/combine multiple datasets)\n",
    "4. Data Cleaning (remove noise and inconsistencies)\n",
    "    * Dealing with missing values\n",
    "    * Dealing with incorrect values (e.g., misspelled names, values out of range)\n",
    "5. Data Transformation (normalize/scale, to discrete values, etc.)\n",
    "\n",
    "It is important for data mining that your process is transparent and repeatable:\n",
    "* Can repeat \"experiment\" and get the same result\n",
    "* No \"magic\" steps\n",
    "\n",
    "It is important, however, to write down steps (log):\n",
    "* Ideally, someone should be able to take your data, program, and description of steps, rerun everything, and get the same results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "It is not uncommon to have datasets with noisy, invalid, or completely missing values.\n",
    "1. Noisy vs Invalid Values\n",
    "    * Noisy implies the value is correct, just recorded incorrectly\n",
    "        * E.g., decimal place error (5.72 instead of 57.2), wrong categorical value used\n",
    "    * Invalid implies a noisy value that is not a valid value (for domain)\n",
    "        * E.g., 57.2X, misspelled categorical data, or value out of range (6 on a 5 point scale)\n",
    "    * Ways to deal with this:\n",
    "        * Look for duplicates (when there shouldn't be)\n",
    "        * Look for outliers\n",
    "        * Sort and print range of values\n",
    "    * The term \"noisy\" may also imply random error or random variance\n",
    "        * Various techniques to \"smooth out\" values\n",
    "        * E.g., using means of bins or regression\n",
    "2. Missing Values\n",
    "     * How should we deal with missing values?\n",
    "        * Discard instances: throw out any row with a missing value\n",
    "        * Replace with a new value:\n",
    "            * By hand\n",
    "            * Use a constant\n",
    "            * Use a central tendency measure (mean, median, most frequent, ...)\n",
    "        * Most \"probable\" value (e.g., regression, using a classifier)\n",
    "        * Replace either across data set, or based on similar instances\n",
    "            * E.g. average based on model year\n",
    "            \n",
    "Missing values are usually coded as an out of range value, such as an empty string in a text field, -1 in a numeric field that is normally positive, or 0 in a numeric field that cannot take on the value of 0. In the Scipy ecosystem, the common value `NaN` (not a number) is used to denote missing data. There is support in the Scipy libraries to handle `NaN` specially. For example, the Pandas function [`isnull()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html) returns a Boolean array detecting the `NaN` values element-wise and [`dropna()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html) removes `NaN` values from a series or data frame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Cleaning Example\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "x = np.arange(0, 10)\n",
    "ser = pd.Series(x)\n",
    "ser[1] = np.NaN\n",
    "ser[5] = np.NaN\n",
    "nans = ser.isnull()\n",
    "# count the number of missing values\n",
    "print(nans.sum())\n",
    "print(ser)\n",
    "ser.dropna(inplace=True)\n",
    "print(ser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can learn more about missing data by reading [Pandas website](https://pandas.pydata.org/pandas-docs/stable/missing_data.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By learning how to use the Pandas library, we have the skills to perform many of the tasks listed above. In this lesson, we are going to focus on *data cleaning*, modifying the data to make it sufficiently accurate and structured to support the analysis you want to perform. To learn about data cleaning, we are going to clean data by working through an example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aggregation\n",
    "Gathering and summarizing data, perhaps in preparation for statistical analysis or visualization, is called *data aggregation*. For example, suppose you want to investigate the similarities/differences amongst patients in a clinical setting. Suppose specific attributes you are interested in include medical condition, age, and gender. You might *group* the data into two groups by gender: male and female. The grouping allows you to then compute statistics such as the mean and standard deviation for each group, perform hypothesis testing to see if there is a significant age difference between the two groups, or perhaps create a bar chart representing the frequency of each medical condition present in each group. \n",
    "\n",
    "## Split-Apply-Combine\n",
    "Data aggregation typically involves a \"split, apply, combine\" process:\n",
    "* Split the data into groups based on some criteria\n",
    "    * Perform *group by* operations\n",
    "    * Select or slice data to form a subset\n",
    "    * Example: Group a data frame by rows (axis 0) or by columns (axis 1)\n",
    "* Apply a function to each group independently, producing a new value\n",
    "    * Compute summary statistics (aggregation)\n",
    "        * Example: Count the size of each group\n",
    "        * Example: Compute mean, standard deviation, custom stats, etc.\n",
    "    * Transform the data in the group (transformation)\n",
    "        * Example: Standardizing data (z-score) within each group\n",
    "        * Example: Filling missing data with a value derived from each group\n",
    "    * Discard some groups (filtration)\n",
    "        * Example: Discarding data that belongs to groups with only a few members\n",
    "        * Example: Filtering out data based on the group sum or mean\n",
    "* Combine the results of the function applications into a data structure\n",
    "    * Example: A series with index corresponding to data frame column names and values representing the column means\n",
    "    \n",
    "<img src=\"https://miro.medium.com/max/1400/1*w2oGdXv5btEMxAkAsz8fbg.png\" width=\"500\">\n",
    "\n",
    "(image from [https://miro.medium.com/max/1400/1*w2oGdXv5btEMxAkAsz8fbg.png](https://miro.medium.com/max/1400/1*w2oGdXv5btEMxAkAsz8fbg.png))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By\n",
    "In the split step, we want to divide a dataset into a mapping of group names to group data. Typically to create the groups, you'll perform a \"group by\" operation. A group by operation is grouping (or partitioning) rows or attributes values by another attribute value.\n",
    "\n",
    "For example, you might have a table like the following:\n",
    "\n",
    "CarName |ModelYear |MSRP\n",
    "-|-|-\n",
    "ford pinto |75 |2769\n",
    "toyota corolla |75 |2711\n",
    "ford pinto |76|3025\n",
    "toyota corolla|77 |2789\n",
    "\n",
    "Let's group the rows by ModelYear, meaning putting all the cars from the year 1975 in one list, all of the cars from the year 1976 in one list, etc. This would create the following partitions (sub tables):\n",
    "\n",
    "CarName |ModelYear |MSRP\n",
    "-|-|-\n",
    "ford pinto |75 |2769\n",
    "toyota corolla |75 |2711\n",
    "\n",
    "\n",
    "CarName |ModelYear |MSRP\n",
    "-|-|-\n",
    "ford pinto |76|3025\n",
    "\n",
    "\n",
    "CarName |ModelYear |MSRP\n",
    "-|-|-\n",
    "toyota corolla|77 |2789\n",
    "\n",
    "\n",
    "Then extract the MPG from each list to get a set of different MPG series, one for each year. Then you could visualize the data with model year on the x-axis, MPG on the y-axis, and one box and whisker for each model year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Pandas [`groupby()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html) function, we can divide a data frame into a [`GroupBy`](http://pandas.pydata.org/pandas-docs/stable/groupby.html) object that stores the mapping. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gender AgeGroup  Feature1  Feature2\n",
      "0      F       OA -0.411167 -1.794967\n",
      "1      F        A  1.086750  0.089496\n",
      "2      M       OA  0.010828 -1.840485\n",
      "3      F       YA  1.127685  2.047733\n",
      "4      M       YA  0.793994  0.758327\n",
      "5      M       OA  1.967862 -0.099171\n",
      "6      M        A  1.106403  0.323073\n",
      "7      F       YA  0.256400 -0.606352\n",
      "Groups: {'F': Int64Index([0, 1, 3, 7], dtype='int64'), 'M': Int64Index([2, 4, 5, 6], dtype='int64')}\n",
      "Female data frame\n",
      "  Gender AgeGroup  Feature1  Feature2\n",
      "0      F       OA -0.411167 -1.794967\n",
      "1      F        A  1.086750  0.089496\n",
      "3      F       YA  1.127685  2.047733\n",
      "7      F       YA  0.256400 -0.606352\n",
      "Male data frame\n",
      "  Gender AgeGroup  Feature1  Feature2\n",
      "2      M       OA  0.010828 -1.840485\n",
      "4      M       YA  0.793994  0.758327\n",
      "5      M       OA  1.967862 -0.099171\n",
      "6      M        A  1.106403  0.323073\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# adapted from http://pandas.pydata.org/pandas-docs/stable/groupby.html\n",
    "df = pd.DataFrame({\"Gender\" : [\"F\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\", \"F\"],\n",
    "                   \"AgeGroup\" : [\"OA\", \"A\", \"OA\", \"YA\", \"YA\", \"OA\", \"A\", \"YA\"], # OA: older adult, A: adult, YA: young adult\n",
    "                   \"Feature1\" : np.random.randn(8),\n",
    "                   \"Feature2\" : np.random.randn(8)})\n",
    "print(df)\n",
    "# GroupBy object (mapping of group name -> group data frame)\n",
    "gender_groups = df.groupby(\"Gender\")\n",
    "# groups attribute is a dictionary storing the mapping\n",
    "print(\"Groups:\", gender_groups.groups)\n",
    "print(\"Female data frame\")\n",
    "F_df = gender_groups.get_group(\"F\")\n",
    "print(F_df)\n",
    "print(\"Male data frame\")\n",
    "M_df = gender_groups.get_group(\"M\")\n",
    "print(M_df)\n",
    "# confirm M_df is a data frame\n",
    "print(type(M_df))\n",
    "# divided the data frame into 2 groups\n",
    "print(len(df) == len(F_df) + len(M_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have learned enough background information to dive into learning about aggregating data by working through an example!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
